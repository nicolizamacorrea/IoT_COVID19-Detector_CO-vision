{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"video.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyPpvIz+3eAJH+LHcTlJSRG/"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vfNBl_CjzZVW"},"source":["from google.colab import drive\n","drive.mount('./MyDrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUzZXgT2ztTl"},"source":["cd /content/MyDrive/My Drive/Colab Notebooks/mask-detection-master"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ssap36mFzTW4"},"source":["#Data Set을 만들어서 학습.\n","from google.colab.patches import cv2_imshow\n","\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","\n","facenet = cv2.dnn.readNet('models/deploy.prototxt', 'models/res10_300x300_ssd_iter_140000.caffemodel')\n","#FaceDetector 모델 > OpenCv의 DNN\n","model = load_model('models/mask_detector.model')\n","#MaskDetector 모델 > Keras 모델\n","\n","cap = cv2.VideoCapture('imgs/01.mp4')\n","#동영상 로드\n","ret, img = cap.read()\n","\n","fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n","out = cv2.VideoWriter('output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (img.shape[1], img.shape[0]))\n","\n","while cap.isOpened():\n","    ret, img = cap.read()\n","    if not ret:\n","        break\n","\n","    h, w = img.shape[:2]\n","\n","    #Preprocessing. OpenCV의 FaceNet에서 학습시킨대로 Param값을 넣어줌. DNN이 사용하는 형태로 이미지 변환\n","    blob = cv2.dnn.blobFromImage(img, scalefactor=1., size=(300, 300), mean=(104., 177., 123.))\n","    facenet.setInput(blob) #변환해준 이미지 FaceNet의 input\n","    dets = facenet.forward() #facedection 결과 저장\n","\n","    result_img = img.copy()\n","\n","    for i in range(dets.shape[2]): #저장이 된 것을 loop을 돌면서 저장\n","        confidence = dets[0, 0, i, 2] #검사하는데 detection의 결과가 자신있는 정도.\n","        if confidence < 0.5:\n","            continue\n","\n","        x1 = int(dets[0, 0, i, 3] * w) #bounding 박스 구해주기\n","        y1 = int(dets[0, 0, i, 4] * h)\n","        x2 = int(dets[0, 0, i, 5] * w)\n","        y2 = int(dets[0, 0, i, 6] * h)\n","        \n","        face = img[y1:y2, x1:x2] #bounding Box을 통해 얼굴만 저장\n","\n","        #마스크를 썼나 안썼나 예측\n","        # 전처리하는 부분\n","        face_input = cv2.resize(face, dsize=(224, 224)) #이미지 크기 변경\n","        face_input = cv2.cvtColor(face_input, cv2.COLOR_BGR2RGB) #이미지의 컬러시스템 변경\n","        face_input = preprocess_input(face_input) #mobileNetV2에서 하는 preprocessing과 똑같이 하기위해 처리\n","        face_input = np.expand_dims(face_input, axis=0)  #이렇게 하면 shape이 (224,224,3) 으로 나오는데 넣을때는 (1,224,224,3)이 되어야 하므로 차원하나 추가\n","        \n","        mask, nomask = model.predict(face_input).squeeze() #load해놓은 모델에 predict method를 통해, 마스크 여부 확률을 반환\n","\n","        if mask > nomask:\n","            color = (0, 255, 0)\n","            label = 'Mask %d%%' % (mask * 100)\n","        else:\n","            color = (0, 0, 255)\n","            label = 'No Mask %d%%' % (nomask * 100)\n","\n","        cv2.rectangle(result_img, pt1=(x1, y1), pt2=(x2, y2), thickness=2, color=color, lineType=cv2.LINE_AA)\n","        cv2.putText(result_img, text=label, org=(x1, y1 - 10), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.8, color=color, thickness=2, lineType=cv2.LINE_AA)\n","\n","    out.write(result_img)\n","    cv2_imshow(result_img)\n","    #cv2.imshow('result', result_img)\n","    if cv2.waitKey(1) == ord('q'):\n","        break\n","\n","out.release()\n","cap.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TWOOcwxYpTZ"},"source":[""],"execution_count":null,"outputs":[]}]}